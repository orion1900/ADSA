{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs and DataFrames\n",
    "\n",
    "* Creating RDDs and DataFrames using SparkContext\n",
    "* Interoperability between RDDs and DataFrames\n",
    "* Multiple rows and multiple column specifications for DataFrames\n",
    "* Creating DataFrames using SQLContext\n",
    "* Selecting, editing and renaming columns in dataframes\n",
    "* Interoperability between Pandas and Spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-833e448fd5ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# check if Spark works after install on local machine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# sc = SparkContext(\"local\",\"test\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sc.parallelize([1,2,3,4])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# sc.stop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# check if Spark works after install on local machine\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# sc = SparkContext(\"local\",\"test\")\n",
    "# sc.parallelize([1,2,3,4])\n",
    "# sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.17763.737]\r\n",
      "(c) 2018 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(base) C:\\Users\\NL27514\\Documents\\Atos\\Projects\\ITPH\\Wk 2 D 2\\Pyspark>java -version\n",
      "\r\n",
      "(base) C:\\Users\\NL27514\\Documents\\Atos\\Projects\\ITPH\\Wk 2 D 2\\Pyspark>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'java' is not recognized as an internal or external command,\r\n",
      "operable program or batch file.\r\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST connection to cluster running on VM\n",
    "\n",
    "# go to the spark web UI in the browser and search for spark.driver.host and spark.driver.port under the Environment tab\n",
    "conf = SparkConf().setAppName('hello2').setMaster('spark://192.168.56.101:42691')  # fill found info here\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"spark://192.168.56.101:7070\") \\\n",
    "    .appName(\"Day1\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use SparkContext. Normally, SparkSession is used.\n",
    "Sparksession encapsulated Sparkcontext.\n",
    "- simplified entry point\n",
    "- no confusion on which context to uses (sql, Hives etc)\n",
    "![](sparksession.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDDs using sc.parallelize()\n",
    "In this example different types of data are used, being string and integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = sc.parallelize([1, \"Alice\", 50])\n",
    "simple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n",
    "\n",
    "<p>data = [1, 2, 3, 4, 5] </p>\n",
    "\n",
    "distData = sc.parallelize(data)\n",
    "\n",
    "Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list. We describe operations on distributed datasets later on.\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](count.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: Count() en First() are ACTIONS on the RDD dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(takes the first 2 elements of the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](collect.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collect Action, shows all the elements in de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERROR\n",
    "\n",
    "* This RDD does not have \"columns\", it cannot be represented as a tabular data frame\n",
    "* DataFrames are structured datasets\n",
    "* So, instead of working with RDD, we want to work with Dataframes.\n",
    "\n",
    "Error: This RDD has NO schema, contains elements of different types - it cannot be converted to a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simple_data.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDDs with records using sc.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = sc.parallelize([[1, \"Alice\", 50], [2, \"Bob\", 80]])\n",
    "# Notice: this has 2 record, one for Alice, and one for Bob\n",
    "records  \n",
    "# The RDD is now created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = records.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "# Column names have been automatically generated and assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "records = spark.sparkContext.parallelize([[1, \"Alice\", 50], [2, \"Bob\", 80]])\n",
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SET -v\").show(n=200, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method show will present the first 20 (by default) rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframes using sc.parallelize() and Row() functions\n",
    "* Row functions allow specifying column names for dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([Row(id=1,\n",
    "                           name=\"Alice\",\n",
    "                           score=50)])\n",
    "# Row is imported in the first lines of this notebook\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So: Structured data with a SCHEMA can be converted to a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([Row(\n",
    "                           id=1,\n",
    "                           name=\"Alice\",\n",
    "                           score=50\n",
    "                        ),\n",
    "                        Row(\n",
    "                            id=2,\n",
    "                            name=\"Bob\",\n",
    "                            score=80\n",
    "                        ),\n",
    "                        Row(\n",
    "                            id=3,\n",
    "                            name=\"Charlee\",\n",
    "                            score=75\n",
    "                        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple columns with complex data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                                col_float=1.44,\n",
    "                                col_integer=10,\n",
    "                                col_string=\"John\")\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                                col_float=1.44, \n",
    "                                col_integer=10, \n",
    "                                col_string=\"John\", \n",
    "                                col_boolean=True, \n",
    "                                col_list=[1, 2, 3])\n",
    "                           ])\n",
    "# The last Column is a List...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                                col_list = [1, 2, 3], \n",
    "                                col_dict = {\"k1\": 0, \"k2\": 1, \"k3\": 2}, \n",
    "                                col_row = Row(columnA = 10, columnB = 20, columnC = 30), \n",
    "                                col_time = datetime(2014, 8, 1, 14, 1, 5)\n",
    "                            )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple rows with complex data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                                col_list = [1, 2, 3],\n",
    "                                col_dict = {\"k1\": 0},\n",
    "                                col_row = Row(a=10, b=20, c=30),\n",
    "                                col_time = datetime(2014, 8, 1, 14, 1, 5)\n",
    "                            ),              \n",
    "                            Row(\n",
    "                                col_list = [1, 2, 3, 4, 5], \n",
    "                                col_dict = {\"k1\": 0,\"k2\": 1 }, \n",
    "                                col_row = Row(a=40, b=50, c=60),\n",
    "                                col_time = datetime(2014, 8, 2, 14, 1, 6)\n",
    "                            ),\n",
    "                            Row(\n",
    "                                col_list = [1, 2, 3, 4, 5, 6, 7], \n",
    "                                col_dict = {\"k1\": 0, \"k2\": 1, \"k3\": 2 }, \n",
    "                                col_row = Row(a=70, b=80, c=90),\n",
    "                                col_time = datetime(2014, 8, 3, 14, 1, 7)\n",
    "                            )]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames using SQLContext\n",
    "\n",
    "* SQLContext can create dataframes directly from raw data\n",
    "\n",
    "https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\n",
    "\n",
    "<p>Example</p>:\n",
    "<p>Create directly from JSON</p>\n",
    "<p>sqlContext = SQLContext(sc)</p>\n",
    "<p>df = sqlContext.read.json(\"examples/src/main/resources/people.json\")</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlContext  = SQLContext(spark.sparkContext)\n",
    "sqlContext = SQLContext(sc)\n",
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.range(5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows specified in tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Alice', 50),\n",
    "        ('Bob', 80),\n",
    "        ('Charlee', 75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify explicitely column names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(data, ['Name', 'Score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data = [\n",
    "                 (1.0,\n",
    "                  10,\n",
    "                  \"Alice\", \n",
    "                  True, \n",
    "                  [1, 2, 3], \n",
    "                  {\"k1\": 0},\n",
    "                  Row(a=1, b=2, c=3), \n",
    "                  datetime(2014, 8, 1, 14, 1, 5)),\n",
    "\n",
    "                 (2.0,\n",
    "                  20,\n",
    "                  \"Bob\", \n",
    "                  True, \n",
    "                  [1, 2, 3, 4, 5], \n",
    "                  {\"k1\": 0,\"k2\": 1 }, \n",
    "                  Row(a=1, b=2, c=3), \n",
    "                  datetime(2014, 8, 1, 14, 1, 5)),\n",
    "\n",
    "                  (3.0,\n",
    "                   30,\n",
    "                   \"Charlee\", \n",
    "                   False, \n",
    "                   [1, 2, 3, 4, 5, 6], \n",
    "                   {\"k1\": 0, \"k2\": 1, \"k3\": 2 }, \n",
    "                   Row(a=1, b=2, c=3), \n",
    "                   datetime(2014, 8, 1, 14, 1, 5))\n",
    "                ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(complex_data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df = sqlContext.createDataFrame(complex_data, [\n",
    "        'col_integer',\n",
    "        'col_float',\n",
    "        'col_string',\n",
    "        'col_boolean',\n",
    "        'col_list',\n",
    "        'col_dictionary',\n",
    "        'col_row',\n",
    "        'col_date_time']\n",
    "    )\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframes using SQL Context and the Row function\n",
    "* Row functions can be used without specifying column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([\n",
    "    Row(1, \"Alice\", 50),\n",
    "    Row(2, \"Bob\", 80),\n",
    "    Row(3, \"Charlee\", 75)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always setup column names, as explained below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = Row('id', 'name', 'score')  \n",
    "students = data.map(lambda r: column_names(*r))\n",
    "# Lambda's : functional programming within on OO world....VERY HANDY, but VERY confusing.....\n",
    "\n",
    "# The map() operations performs a transformation on every element in the RDD.qw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = sqlContext.createDataFrame(students)\n",
    "students_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting specific rows from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting specific cells from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_string = complex_data_df.collect()[0][2]\n",
    "cell_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_list = complex_data_df.collect()[0][4]\n",
    "cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_list.append(100)\n",
    "cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The orignional dataframe has not been modified\n",
    "# When you access a cell, that cell will be unchanged\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting specific columns\n",
    "\n",
    "Every Dataframe contains the RDD equivalent of the record that it stores, in the RDD variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.rdd\\\n",
    "    .map(lambda x: (x.col_string, x.col_dictionary))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.select(\n",
    "    'col_string',\n",
    "    'col_list',\n",
    "    'col_date_time'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A map() operation which appends \"Boo\" to every string in the column\n",
    "complex_data_df.rdd\\\n",
    "           .map(lambda x: (x.col_string + \" Boo\"))\\\n",
    "           .collect()\n",
    "# Dataframes do not support the add function. To do this, you must create a new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.select(\n",
    "                   'col_integer',\n",
    "                   'col_float'\n",
    "            )\\\n",
    "           .withColumn(\n",
    "                   \"col_sum\",\n",
    "                    complex_data_df.col_integer + complex_data_df.col_float\n",
    "           )\\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, producing the opposite boolean column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.select('col_boolean')\\\n",
    "               .withColumn(\n",
    "                   \"col_opposite\",\n",
    "                   complex_data_df.col_boolean == False )\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing a column name / Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.withColumnRenamed(\"col_dictionary\",\"col_map\").show()\n",
    "# A new dataframe is actually made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data_df.select(complex_data_df.col_string.alias(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interoperablity between Pandas dataframe and Spark dataframe\n",
    "Remember: RDD and Dataframes are distributed accros nodes. Pandas will be on ONE machine (in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = complex_data_df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Pandas to Dataframe...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = sqlContext.createDataFrame(df_pandas).show()  \n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
